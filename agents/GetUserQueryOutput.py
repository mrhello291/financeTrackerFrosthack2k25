import json
from langchain_core.prompts import ChatPromptTemplate
# from langchain_groq import ChatGroq
# from langchain_huggingface import HuggingFaceEndpoint
from dotenv import load_dotenv, find_dotenv
import os
# from langchain_google_genai import ChatGoogleGenerativeAI
from utils.Score_RAG_Summarizer import run
from utils.asiChat import llmChat
from uagents import Agent, Bureau, Context, Model
from utils.DriveJSONRetriever import retrieve_data_from_gdrive


load_dotenv(find_dotenv())
os.environ['LANGCHAIN_TRACING_V2'] = 'true'
os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'
os.environ['LANGCHAIN_PROJECT'] = 'advanced-rag'
os.environ['LANGCHAIN_API_KEY'] = os.getenv("LANGSMITH_API_KEY")
# os.environ['GROQ_API_KEY'] = os.getenv("GROQ_API_KEY")
# os.environ["HUGGINGFACEHUB_API_TOKEN"] = os.getenv("HUGGINGFACEHUB_API_TOKEN")
# os.environ["GOOGLE_API_KEY"] = os.getenv("GEMINI_API_KEY")


def answerQuery(user_query, filtered_transactions):
    # ans = run(user_query)['content']

    transactions_context = json.dumps(filtered_transactions, indent=4)

    prompt_template = ChatPromptTemplate.from_messages([
        ("system", "You are analyzing a list of financial transactions. Each transaction contains Date, Particulars, Deposit, Withdrawal, and Balance. "
                "Spendings can be observed in the Withdrawal column, while earnings can be observed in the Deposit column. "
                "Answer the user's question based on the given transactions. Respond accurately based only on the provided data."),
        ("user", "Transactions Data:\n{transactions}\n\nUser Query: {query}"),
        # ("assistant", "The following contextual summary, generated by a separate summarizing agent which has an overview of whole document"
        #             "It is provided solely as background information to give answers to general financial questions. It might or might not be true."
        #             "Use this information only if it naturally supports or reinforces your answer, and do not let it override the core facts from the transactions.")
    ])


    prompt = prompt_template.format_messages(transactions=transactions_context, query=user_query)
    # response = model.invoke(prompt)
    response = llmChat(prompt)

    # Print the response
    print("LLM Response:", response)
    return response


class QueryAnswerAgentMessage(Model):
    message: str
    # query : str
    # fld : dict

class QueryAnswerAgentMessageResponse(Model):
    ans: str
    
QueryAnswerAgent = Agent(name="QueryAnswerAgent", seed="QueryAnswerAgent recovery phrase", port=8004, mailbox=True)

@QueryAnswerAgent.on_rest_post("/pest/post", QueryAnswerAgentMessage, QueryAnswerAgentMessageResponse)
async def query_answer_agent(ctx: Context , message: QueryAnswerAgentMessage) -> QueryAnswerAgentMessageResponse:
    """
    Handles the query answer agent's message.

    Args:
        context (Context): The context of the agent.
        sender (str): The sender of the message.
        message (InputReaderAgentMessage): The message from the query answer agent.
    """
    
    print("\n ------Getting relevant transactions---------. \n")
    fld2= {}
    with open("INFO/filtered_transactions.json", "r") as file:
        fld2 = json.load(file)
    # fld2 = retrieve_data_from_gdrive('filtered_transactions.json')
    print("\n ------Getting answer to the query---------. \n")
    ans = answerQuery(message.message, fld2)
    print("\n ------Got answer to the query successfully---------. \n")
    
    # await ctx.send(sender, ans)
    return QueryAnswerAgentMessageResponse(ans=ans)


if __name__ == "__main__":
    # Start the QueryAnswerAgent
    QueryAnswerAgent.run()
